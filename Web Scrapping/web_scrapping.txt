import csv
fw = open("a.csv","w",encoding="utf-8")
fw.write("RNO, NAME, PER \n")
fw.write("101, AAA, 75 \n")
fw.write("102, BBB, 85 \n")
fw.close()
print("File Created")


-------------------------------------------------------



from bs4 import BeautifulSoup

html_page = "<html> <body> <h1> Techno Comp Academy </h1> </body> </html> "

bs = BeautifulSoup(html_page,"html.parser")

p_tag = bs.find("h1")
print(p_tag.text)


-------------------------------------------------------


from bs4 import BeautifulSoup

f = open("demo.html","r")

html_page = f.read()

bs = BeautifulSoup(html_page,"html.parser")

p_tags = bs.find_all("p")

for p in p_tags:
	print(p.text)


-------------------------------------------------------


import urllib.request

url = "https://www.wikipedia.org"

response = urllib.request.urlopen(url)

bdata = response.read()

data = bdata.decode("utf-8")

print(data)


-------------------------------------------------------


import urllib.request

url = "https://www.wikipedia.org"

response = urllib.request.urlopen(url)
#print(response)

bdata = response.read()
#print(bdata)

data = bdata.decode("utf-8")

print(type(data))
#L = data.split("<div>")

print(data)


-------------------------------------------------------


import urllib.request
from bs4 import BeautifulSoup


#url = "https://webscraper.io/test-sites"

url = "https://webscraper.io/test-sites/e-commerce/allinone"

response = urllib.request.urlopen(url)

bdata = response.read()

data = bdata.decode("utf-8")

bs = BeautifulSoup(data,"html.parser")

li_tag = bs.find_all("li")

for i in li_tag:
	print(i.text)




-------------------------------------------------------



<a class="_1fQZEK" target="_blank" rel="noopener noreferrer" href="/oneplus-nord-2t-5g-gray-shadow-128-gb/p/itmeadf76c1e8943?pid=MOBGHBZH2S8MWQGQ&amp;lid=LSTMOBGHBZH2S8MWQGQJUSP9D&amp;marketplace=FLIPKART&amp;q=oneplusnord2t&amp;store=tyy%2F4io&amp;srno=s_1_1&amp;otracker=search&amp;otracker1=search&amp;fm=organic&amp;iid=f80756c5-3d93-4f2e-bfdb-42af1c615a0d.MOBGHBZH2S8MWQGQ.SEARCH&amp;ppt=hp&amp;ppn=homepage&amp;ssid=upe4xpa8yo0000001694935466716&amp;qH=4e37e9b319184b77"><div class="MIXNux"><div class="_2QcLo-"><div><div class="CXW8mj" style="height: 200px; width: 200px;"><img loading="eager" class="_396cs4" alt="OnePlus Nord 2T 5G (Gray Shadow, 128 GB)" src="https://rukminim2.flixcart.com/image/312/312/xif0q/mobile/f/a/v/-original-imagfx6b9shazw8u.jpeg?q=70"></div></div></div><div class="_3wLduG"><div class="_3PzNI-"><span class="f3A4_V"><label class="_2iDkf8"><input type="checkbox" class="_30VH1S" readonly=""><div class="_24_Dny"></div></label></span><label class="_6Up2sF"><span>Add to Compare</span></label></div></div><div class="_2hVSre _3nq8ih"><div class="_36FSn5"><svg xmlns="http://www.w3.org/2000/svg" class="_1l0elc" width="16" height="16" viewBox="0 0 20 16"><path d="M8.695 16.682C4.06 12.382 1 9.536 1 6.065 1 3.219 3.178 1 5.95 1c1.566 0 3.069.746 4.05 1.915C10.981 1.745 12.484 1 14.05 1 16.822 1 19 3.22 19 6.065c0 3.471-3.06 6.316-7.695 10.617L10 17.897l-1.305-1.215z" fill="#2874F0" class="eX72wL" stroke="#FFF" fill-rule="evenodd" opacity=".9"></path></svg></div></div></div><div class="_3pLy-c row"><div class="col col-7-12"><div class="_4rR01T">OnePlus Nord 2T 5G (Gray Shadow, 128 GB)</div><div class="gUuXy-"><span id="productRating_LSTMOBGHBZH2S8MWQGQJUSP9D_MOBGHBZH2S8MWQGQ_" class="_1lRcqv"><div class="_3LWZlK">4.4<img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMyIgaGVpZ2h0PSIxMiI+PHBhdGggZmlsbD0iI0ZGRiIgZD0iTTYuNSA5LjQzOWwtMy42NzQgMi4yMy45NC00LjI2LTMuMjEtMi44ODMgNC4yNTQtLjQwNEw2LjUuMTEybDEuNjkgNC4wMSA0LjI1NC40MDQtMy4yMSAyLjg4Mi45NCA0LjI2eiIvPjwvc3ZnPg==" class="_1wB99o"></div></span><span class="_2_R_DZ"><span><span>12,079 Ratings&nbsp;</span><span class="_13vcmD">&amp;</span><span>&nbsp;965 Reviews</span></span></span></div><div class="fMghEO"><ul class="_1xgFaf"><li class="rgWa7D">8 GB RAM | 128 GB ROM</li><li class="rgWa7D">17.02 cm (6.7 inch) Display</li><li class="rgWa7D">50MP Rear Camera | 32MP Front Camera</li><li class="rgWa7D">4500 mAh Battery</li><li class="rgWa7D">12 Month</li></ul></div></div><div class="col col-5-12 nlI3QM"><div class="_3tbKJL"><div class="_25b18c"><div class="_30jeq3 _1_WHN1">₹28,990</div><div class="_3I9_wc _27UcVY">₹28,999</div></div><div class="_3tcB5a p8ucoS"><div><div class="_2Tpdn3" style="color: rgb(0, 0, 0); font-size: 12px; font-weight: 400;">Free delivery</div></div></div></div><div class="_13J9qT"><img height="21" src="//static-assets-web.flixcart.com/fk-p-linchpin-web/fk-cp-zion/img/fa_62673a.png"></div><div class="_2ZdXDB"><div class="_3xFhiH"><div class="_2Tpdn3 _18hQoS" style="color: rgb(38, 165, 65); font-size: 14px; font-weight: 700;">Bank Offer</div></div></div></div></div></a>



-------------------------------------------------------

from bs4 import BeautifulSoup
from urllib.request import urlopen
import requests


search_str = input("Which product you want to search : ")
filename = search_str+".csv"

fw = open(filename,"w",encoding="utf-8")
header = "Product, Customer Name, Rating, Heading, Comment \n"
fw.write(header)


flipkart_url = "https://www.flipkart.com/search?q=" + search_str

url_req = urlopen(flipkart_url)
flipkart_page = url_req.read()
url_req.close()

flipkart_html = BeautifulSoup(flipkart_page,"html.parser")

boxes = flipkart_html.find_all("div",{"class" : "_1AtVbE col-12-12"})

del boxes[0:3]
box = boxes[0]

productLink = "https://www.flipkart.com" + box.div.div.div.a['href']

prod_res = requests.get(productLink)
prod_res.encoding = "utf-8"

prod_html = BeautifulSoup(prod_res.text,"html.parser")



commentboxes = prod_html.find_all("div",{"class":"_16PBlm"})

del commentboxes[-1]

for commentbox in commentboxes:
	try:
		name = commentbox.div.div.find_all("p",{"class": "_2-N8zT"})
		commenthead = name[0].text

		rating = commentbox.div.div.div.div.text
		
		name = commentbox.div.div.find_all("p",{"class" : "_2sc7ZR _2V5EHH"})[0].text

		comtag = commentbox.div.div.find_all("div",{"class": ""})
		custcomment = comtag[0].div.text		


		fw.write(search_str+", "+name+", "+rating+", "+commenthead+", "+custcomment+" \n")

	except Exception as e:
		print(e)


fw.close()
print(search_str+".csv is  created !!!")


-------------------------------------------------------

https://webscraper.io/images/test-sites/e-commerce/items/cart2.png
https://webscraper.io/test-sites/images/test-sites/e-commerce/items/cart2.png


https://webscraper.io/test-sites/e-commerce/allinone/images/test-sites/e-commerce/items/cart2.png

-------------------------------------------------------

from bs4 import BeautifulSoup
from urllib.request import urlopen
import requests

url = "https://webscraper.io/test-sites/e-commerce/allinone"

url_req = urlopen(url)
source_page = url_req.read()
url_req.close()

page_html = BeautifulSoup(source_page,"html.parser")
value = page_html.find("div",{"class":"col-sm-4 col-lg-4 col-md-4"})

title = value.div.img['src']
img = "https://webscraper.io" + title
print(img)
url_req = urlopen(img)
source_page = url_req.read()

f = open("a.png","w")
f.write(str(source_page))
f.close()
print("Done")





-------------------------------------------------------

from bs4 import BeautifulSoup 
from urllib.request import urlopen

url = "https://www.flipkart.com"
url_res = urlopen(url)
page_source = url_res.read()
url_res.close()

html_page = BeautifulSoup(page_source,"html.parser")

mobile = html_page.find_all("div",{"class":"_3YgSsQ"})
for i in mobile:
	print(i)


-------------------------------------------------------

from bs4 import BeautifulSoup 
from urllib.request import urlopen

url = "https://en.wikipedia.org/wiki/Main_Page"
page = urlopen(url)
page_info = page.read()
page.close()

html_text = BeautifulSoup(page_info,"html.parser")

bird_info = html_text.find_all("div",{"class" : "MainPageBG mp-box"})

print(bird_info[0])

-------------------------------------------------------

from bs4 import BeautifulSoup
from urllib.request import urlopen


f = open("top_movies.csv","w")
header = "Movie Name\n"
f.write(header)

url = "https://www.timeout.com/film/best-movies-of-all-time"
url_responce = urlopen(url)
page_info = url_responce.read()
url_responce.close()


page_html_info = BeautifulSoup(page_info,"html.parser")

boxes = page_html_info.find_all("div",{"class":"articleContent _articleContent_kc5qn_216"})

#print(len(boxes))

#print(boxes[0].div.a.h3.text)
for box in boxes:
	f.write(box.div.a.h3.text+"\n")
f.close()


-------------------------------------------------------

from bs4 import BeautifulSoup
from urllib.request import urlopen

url = "https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States"

url_res = urlopen(url)
page_info = url_res.read()
url_res.close()

page_html_info = BeautifulSoup(page_info, "html.parser")

data = page_html_info.find_all("div", {"class": "cn-fundraising"})

for i in data:
    print(i.div.table.body.tr.td.b.a.text)



-------------------------------------------------------

from bs4 import BeautifulSoup
from urllib.request import urlopen

url = "https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States"
url_res = urlopen(url)
page_res = url_res.read()
url_res.close()

html_info = BeautifulSoup(page_res,"html.parser")

presidents = html_info.find_all("div",{"class":"mw-body-content mw-content-ltr"})

#print(presidents[0].div.table.td)

p = presidents[0].div.table.find_all("td",{"class":""})
print(p[1]["data-sort-value"])



-------------------------------------------------------





-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------